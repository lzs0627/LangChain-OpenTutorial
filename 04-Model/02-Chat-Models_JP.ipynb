{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«\n",
    "\n",
    "- è‘—è€…: [PangPangGod](https://github.com/pangpanggod)\n",
    "- ãƒ”ã‚¢ãƒ¬ãƒ“ãƒ¥ãƒ¼ : [YooKyung Jeon](https://github.com/sirena1)\n",
    "- æ ¡æ­£ : [Two-Jay](https://github.com/Two-Jay)\n",
    "- ã“ã‚Œã¯ [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial) ã®ä¸€éƒ¨ã§ã™ã€‚\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/04-Model/02-Chat-Mode[...]\n",
    "## æ¦‚è¦\n",
    "\n",
    "ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€ã•ã¾ã–ã¾ãªãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆOpenAIã€Anthropic ãªã©ï¼‰ã®èª¬æ˜ã¨ç°¡å˜ãªä½¿ç”¨ä¾‹ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚\n",
    "\n",
    "### ç›®æ¬¡\n",
    "\n",
    "- [æ¦‚è¦](#overview)\n",
    "- [ç’°å¢ƒè¨­å®š](#environment-setup)\n",
    "- [OpenAI](#openai)\n",
    "- [Anthropic](#anthropic)\n",
    "- [Perplexity](#perplexity)\n",
    "- [Together AI](#together-ai)\n",
    "- [Cohere](#cohere)\n",
    "- [Upstage](#upstage)\n",
    "- [Open LLM Leaderboard](#open-llm-leaderboard)\n",
    "- [Vellum LLM Leaderboard](#vellum-llm-leaderboard)\n",
    "\n",
    "### å‚è€ƒè³‡æ–™\n",
    "\n",
    "- [OpenAI Model Specifications](https://platform.openai.com/docs/models)\n",
    "- [LangChain ChatOpenAI API reference](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n",
    "- [Anthropic Model Specifications](https://docs.anthropic.com/en/docs/about-claude/models)\n",
    "- [LangChain ChatAnthropic API reference](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html)\n",
    "- [Perplexity Model Cards](https://docs.perplexity.ai/guides/model-cards)\n",
    "- [LangChain ChatPerplexity API reference](https://api.python.langchain.com/en/latest/community/chat_models/langchain_community.chat_models.perplexity.ChatPerplexity.html)\n",
    "- [Together AI Model Specifications](https://api.together.xyz/models)\n",
    "- [LangChain ChatTogether API reference](https://python.langchain.com/api_reference/together/chat_models/langchain_together.chat_models.ChatTogether.html)\n",
    "- [Cohere Model Specifications](https://docs.cohere.com/docs/models)\n",
    "- [LangChain ChatCohere API reference](https://python.langchain.com/api_reference/cohere/chat_models/langchain_cohere.chat_models.ChatCohere.html)\n",
    "- [Upstage Model Specifications](https://console.upstage.ai/docs/capabilities/chat)\n",
    "- [LangChain ChatUpstage API reference](https://python.langchain.com/api_reference/upstage/chat_models/langchain_upstage.chat_models.ChatUpstage.html)\n",
    "- [HuggingFace Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)\n",
    "- [Vellum LLM Leaderboard](https://www.vellum.ai/llm-leaderboard)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç’°å¢ƒè¨­å®š\n",
    "\n",
    "ç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã¾ã™ã€‚è©³ç´°ã¯ [Environment Setup](https://wikidocs.net/257836) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "**[æ³¨æ„]**\n",
    "- ```langchain-opentutorial``` ã¯ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ç”¨ã®ä¾¿åˆ©ãªç’°å¢ƒè¨­å®šã‚„é–¢æ•°ã€ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’æä¾›ã™ã‚‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã§ã™ã€‚\n",
    "- è©³ç´°ã¯ [```langchain-opentutorial```](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_openai\",\n",
    "        \"langchain_anthropic\",\n",
    "        \"langchain_community\",\n",
    "        \"langchain_together\",\n",
    "        \"langchain_cohere\",\n",
    "        \"langchain_upstage\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è‡ªå‹•ã§ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ¼ãƒ«ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’å–å¾—ã—ãŸã„å ´åˆã¯ã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’ã‚¢ãƒ³ã‚³ãƒ¡ãƒ³ãƒˆã—ã¦ LangSmith API ã‚­ãƒ¼ã‚’è¨­å®šã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "\n",
    "# from langchain_opentutorial import set_env\n",
    "\n",
    "# set_env(\n",
    "#     {\n",
    "#         \"LANGCHAIN_API_KEY\": \"\",\n",
    "#         \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "#         \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "#         \"LANGCHAIN_PROJECT\": \"01-Chat-Models\",\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "\n",
    "OpenAI ã¯ã‚µãƒ³ãƒ•ãƒ©ãƒ³ã‚·ã‚¹ã‚³ã‚’æ‹ ç‚¹ã¨ã™ã‚‹ AI ç ”ç©¶ãƒ»å±•é–‹ä¼æ¥­ã§ã€äººå·¥æ±ç”¨çŸ¥èƒ½ï¼ˆAGIï¼‰ãŒå…¨äººé¡ã«åˆ©ç›Šã‚’ã‚‚ãŸã‚‰ã™ã‚ˆã†ã«åŠªã‚ã¦ã„ã¾ã™ã€‚ä»£è¡¨çš„ãªãƒ¢ãƒ‡ãƒ«ã¯ GPT ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚\n",
    "\n",
    "### ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜\n",
    "\n",
    "| ãƒ¢ãƒ‡ãƒ«               | èª¬æ˜                                                                                         | ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·     | æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•° | å­¦ç¿’ãƒ‡ãƒ¼ã‚¿             |\n",
    "|--------------------|---------------------------------------------------------------------------------------------|------------------|-------------------|----------------------|\n",
    "| gpt-4o            | å¤šç”¨é€”ã§é«˜æ€§èƒ½ãªãƒ•ãƒ©ãƒƒã‚°ã‚·ãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«ã€‚GPT-4 Turbo ã‚ˆã‚Šå®‰ä¾¡ã§é«˜é€Ÿã€‚                                | 128,000 ãƒˆãƒ¼ã‚¯ãƒ³  | 16,384 ãƒˆãƒ¼ã‚¯ãƒ³    | 2023å¹´10æœˆã¾ã§        |\n",
    "| chatgpt-4o-latest | ChatGPTã§ä½¿ç”¨ã•ã‚Œã‚‹ç¶™ç¶šçš„ã«æ›´æ–°ã•ã‚Œã‚‹ GPT-4o ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€‚                                     | 128,000 ãƒˆãƒ¼ã‚¯ãƒ³  | 16,384 ãƒˆãƒ¼ã‚¯ãƒ³    | ç¶™ç¶šçš„ã«æ›´æ–°          |\n",
    "| gpt-4o-mini       | GPT-3.5 Turbo ã‚ˆã‚Šæ€§èƒ½ãŒè‰¯ã„ã€å°å‹ã§é«˜é€Ÿãªãƒ¢ãƒ‡ãƒ«ã€‚                                               | 128,000 ãƒˆãƒ¼ã‚¯ãƒ³  | 16,384 ãƒˆãƒ¼ã‚¯ãƒ³    | 2023å¹´10æœˆã¾ã§        |\n",
    "| gpt-4-turbo       | ãƒ“ã‚¸ãƒ§ãƒ³å¯¾å¿œã€JSON ãƒ¢ãƒ¼ãƒ‰ã€é–¢æ•°å‘¼ã³å‡ºã—æ©Ÿèƒ½ã‚’å‚™ãˆãŸæœ€æ–°ã® GPT-4 Turbo ãƒ¢ãƒ‡ãƒ«ã€‚                      | 128,000 ãƒˆãƒ¼ã‚¯ãƒ³  | 4,096 ãƒˆãƒ¼ã‚¯ãƒ³     | 2023å¹´12æœˆã¾ã§        |\n",
    "| gpt-4o-realtime   | éŸ³å£°ã¨ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã«å¯¾å¿œã—ãŸãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ APIå‘ã‘ã®ãƒ™ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã€‚                                    | 128,000 ãƒˆãƒ¼ã‚¯ãƒ³  | 4,096 ãƒˆãƒ¼ã‚¯ãƒ³     | 2023å¹´10æœˆã¾ã§        |\n",
    "| gpt-4o-audio      | Chat Completions API çµŒç”±ã§éŸ³å£°å…¥åŠ›ã¨å‡ºåŠ›ã‚’æ‰±ãˆã‚‹ãƒ™ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã€‚                                   | 128,000 ãƒˆãƒ¼ã‚¯ãƒ³  | 16,384 ãƒˆãƒ¼ã‚¯ãƒ³    | 2023å¹´10æœˆã¾ã§        |\n",
    "| gpt-3.5-turbo     | ä¼šè©±ã‚„éä¼šè©±ã‚¿ã‚¹ã‚¯ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã€è‡ªç„¶è¨€èªã¨ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã«å¼·ã¿ãŒã‚ã‚Šã¾ã™ã€‚                      | 16,385 ãƒˆãƒ¼ã‚¯ãƒ³   | 4,096 ãƒˆãƒ¼ã‚¯ãƒ³     | 2021å¹´9æœˆã¾ã§         |\n",
    "\n",
    "OpenAI ã¯å¤šæ§˜ãªãƒ¢ãƒ‡ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚è©³ç´°ã¯ä»¥ä¸‹ã‚’å‚ç…§ã—ã¦ãã ã•ã„:  \n",
    "[OpenAI Model Specifications](https://platform.openai.com/docs/models)\n",
    "\n",
    "### åŸºæœ¬çš„ãªãƒ¢ãƒ‡ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³\n",
    "\n",
    "åŸºæœ¬çš„ãª API ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯æ¬¡ã®é€šã‚Šã§ã™:\n",
    "\n",
    "- ```model_name``` : ```str```  \n",
    "  ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¾ã™ã€‚```model``` ã¨ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã§ãã¾ã™ã€‚\n",
    "\n",
    "- ```temperature``` : ```float``` = 0.7    \n",
    "  ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®æ¸©åº¦ã‚’è¨­å®šã—ã¾ã™ã€‚å€¤ã¯ 0 ã‹ã‚‰ 2 ã®é–“ã§ã€å€¤ãŒé«˜ã„ã»ã©å‡ºåŠ›ã¯ãƒ©ãƒ³ãƒ€ãƒ ã«ãªã‚Šã¾ã™ï¼ˆä¾‹: 0.8ï¼‰ã€‚ä½ã„å€¤ï¼ˆä¾‹: 0.2ï¼‰ã¯ã‚ˆã‚Šæ±ºå®šçš„ãªå‡ºåŠ›ã‚’ä¿ƒã—ã¾ã™ã€‚\n",
    "\n",
    "- ```max_tokens``` : ```int``` | ```None``` = ```None```    \n",
    "  ãƒãƒ£ãƒƒãƒˆè£œå®Œã§ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚ä¸€åº¦ã«ç”Ÿæˆã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚\n",
    "\n",
    "è©³ç´°ãª API ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ä»¥ä¸‹ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n",
    "https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create a ChatOpenAI object\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",  # Model name (can be aliased as 'model')\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ç’°å¢ƒå¤‰æ•°ã« ```OPENAI_API_KEY``` ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚API ã‚­ãƒ¼ã‚’æ‰‹å‹•ã§æŒ‡å®šã—ãŸã‚Šåˆ¥ãƒ¢ãƒ‡ãƒ«ã‚’é¸ã³ãŸã„å ´åˆã¯ã€ä»¥ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ChatOpenAI(temperature=0, api_key=\"YOUR_API_KEY_HERE\", model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs!"
     ]
    }
   ],
   "source": [
    "query = \"Tell me one joke about Computer Science\"\n",
    "\n",
    "# Stream the response instead of invoking it directly\n",
    "response = model.stream(query)\n",
    "\n",
    "# Print the streamed response token by token\n",
    "for token in response:\n",
    "    print(token.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic\n",
    "\n",
    "Anthropic ã¯ã‚µãƒ³ãƒ•ãƒ©ãƒ³ã‚·ã‚¹ã‚³ã‚’æ‹ ç‚¹ã¨ã™ã‚‹ AI å®‰å…¨æ€§ã¨ç ”ç©¶ã®ä¼šç¤¾ã§ã€ä¿¡é ¼æ€§ãŒé«˜ãè§£é‡ˆå¯èƒ½ã€ã‹ã¤æ“ç¸¦å¯èƒ½ãª AI ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã«æ³¨åŠ›ã—ã¦ã„ã¾ã™ã€‚ä¸»ãªæä¾›ç‰©ã¯ **Claude ç³»** ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã€**Claude 3.5 Sonnet** ã‚„ **Claude 3.5 Haiku** ãªã©ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "### ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜\n",
    "\n",
    "| ãƒ¢ãƒ‡ãƒ«               | èª¬æ˜                                              | ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·     | æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•° | å­¦ç¿’ãƒ‡ãƒ¼ã‚¿           |\n",
    "|--------------------|--------------------------------------------------|------------------|-------------------|----------------------|\n",
    "| Claude 3.5 Sonnet  | Claude ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã§æœ€ã‚‚é«˜æ€§èƒ½ãªãƒ¢ãƒ‡ãƒ«ã€‚             | 200,000 ãƒˆãƒ¼ã‚¯ãƒ³  | 8,192 ãƒˆãƒ¼ã‚¯ãƒ³     | 2024å¹´4æœˆã¾ã§        |\n",
    "| Claude 3.5 Haiku   | æœ€ã‚‚é«˜é€Ÿã§ã€éå¸¸ã«é«˜é€Ÿå¿œç­”ãŒå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã€‚         | 200,000 ãƒˆãƒ¼ã‚¯ãƒ³  | 8,192 ãƒˆãƒ¼ã‚¯ãƒ³     | 2024å¹´7æœˆã¾ã§        |\n",
    "| Claude 3 Opus      | é«˜åº¦ãªè¤‡é›‘ã‚¿ã‚¹ã‚¯å‘ã‘ã®å¼·åŠ›ãªãƒ¢ãƒ‡ãƒ«ã€‚               | 200,000 ãƒˆãƒ¼ã‚¯ãƒ³  | 4,096 ãƒˆãƒ¼ã‚¯ãƒ³     | 2023å¹´8æœˆã¾ã§        |\n",
    "| Claude 3 Sonnet    | ã‚¹ã‚±ãƒ¼ãƒ«ã—ãŸãƒ‡ãƒ—ãƒ­ã‚¤ã«å‘ããƒãƒ©ãƒ³ã‚¹å‹ãƒ¢ãƒ‡ãƒ«ã€‚        | 200,000 ãƒˆãƒ¼ã‚¯ãƒ³  | 4,096 ãƒˆãƒ¼ã‚¯ãƒ³     | 2023å¹´8æœˆã¾ã§        |\n",
    "| Claude 3 Haiku     | åå¿œé€Ÿåº¦ãŒæœ€é€Ÿã§ã€ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãªãƒ¢ãƒ‡ãƒ«ã€‚             | 200,000 ãƒˆãƒ¼ã‚¯ãƒ³  | 4,096 ãƒˆãƒ¼ã‚¯ãƒ³     | 2023å¹´8æœˆã¾ã§        |\n",
    "\n",
    "è©³ç´°ã¯ä»¥ä¸‹ã‚’å‚ç…§ã—ã¦ãã ã•ã„:  \n",
    "[Anthropic Model Specifications](https://docs.anthropic.com/en/docs/about-claude/models)\n",
    "\n",
    "### åŸºæœ¬çš„ãªãƒ¢ãƒ‡ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³\n",
    "\n",
    "åŸºæœ¬çš„ãª API ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯æ¬¡ã®é€šã‚Šã§ã™:\n",
    "\n",
    "- ```model_name``` : ```str```  \n",
    "  ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¾ã™ã€‚```model``` ã¨ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã§ãã¾ã™ã€‚\n",
    "\n",
    "- ```temperature``` : ```float``` = 0.7    \n",
    "  ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®æ¸©åº¦ã‚’è¨­å®šã—ã¾ã™ã€‚å€¤ãŒé«˜ã„ã»ã©å‡ºåŠ›ã¯ã‚ˆã‚Šãƒ©ãƒ³ãƒ€ãƒ ã«ãªã‚Šã€å€¤ãŒä½ã„ã»ã©æ±ºå®šçš„ã«ãªã‚Šã¾ã™ã€‚\n",
    "\n",
    "- ```max_tokens``` : ```int``` | ```None``` = ```None```    \n",
    "  ãƒãƒ£ãƒƒãƒˆè£œå®Œã§ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚\n",
    "\n",
    "è©³ç´°ã¯ä»¥ä¸‹ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n",
    "https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"ANTHROPIC_API_KEY\"):\n",
    "  os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter API key for Anthropic: \")\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Create a ChatAnthropic object\n",
    "model = ChatAnthropic(\n",
    "    model_name=\"claude-3-5-haiku-latest\", # Model name (can be aliased as 'model')\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ç’°å¢ƒå¤‰æ•°ã« ```ANTHROPIC_API_KEY``` ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚æ‰‹å‹•ã§ API ã‚­ãƒ¼ã‚’æŒ‡å®šã—ãŸã‚Šåˆ¥ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ChatAnthropic(temperature=0, api_key=\"YOUR_API_KEY_HERE\", model=\"claude-3-5-haiku-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a classic computer science joke:\n",
      "\n",
      "Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs! ğŸ˜„\n",
      "\n",
      "(This is a play on words, referencing both computer \"bugs\" (errors) and actual insects being attracted to light.)"
     ]
    }
   ],
   "source": [
    "query = \"Tell me one joke about Computer Science\"\n",
    "\n",
    "# Stream the response instead of invoking it directly\n",
    "response = model.stream(query)\n",
    "\n",
    "# Print the streamed response token by token\n",
    "for token in response:\n",
    "    print(token.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "\n",
    "Perplexity AI ã¯ã€é«˜åº¦ãªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’çµ±åˆã—ã¦ã‚½ãƒ¼ã‚¹ã®å¼•ç”¨ä»˜ãã§ç›´æ¥å›ç­”ã‚’æä¾›ã™ã‚‹ä¼šè©±å‹æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã§ã™ã€‚ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã¯ã€ãƒãƒ£ãƒƒãƒˆè£œå®Œã‚¿ã‚¹ã‚¯å‘ã‘ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€æ‹¡å¼µã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "### ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«\n",
    "\n",
    "| ãƒ¢ãƒ‡ãƒ«                              | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·     | ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—        |\n",
    "|------------------------------------|-------------:|------------------:|-------------------|\n",
    "| llama-3.1-sonar-small-128k-online  | 8B           | 127,072 ãƒˆãƒ¼ã‚¯ãƒ³   | ãƒãƒ£ãƒƒãƒˆè£œå®Œ       |\n",
    "| llama-3.1-sonar-large-128k-online  | 70B          | 127,072 ãƒˆãƒ¼ã‚¯ãƒ³   | ãƒãƒ£ãƒƒãƒˆè£œå®Œ       |\n",
    "| llama-3.1-sonar-huge-128k-online   | 405B         | 127,072 ãƒˆãƒ¼ã‚¯ãƒ³   | ãƒãƒ£ãƒƒãƒˆè£œå®Œ       |\n",
    "\n",
    "è©³ç´°ã¯ä»¥ä¸‹ã‚’å‚ç…§ã—ã¦ãã ã•ã„:  \n",
    "[Perplexity Model Cards](https://docs.perplexity.ai/guides/model-cards)\n",
    "\n",
    "### åŸºæœ¬çš„ãªãƒ¢ãƒ‡ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³\n",
    "\n",
    "åŸºæœ¬çš„ãª API ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯æ¬¡ã®é€šã‚Šã§ã™:\n",
    "\n",
    "- ```model``` : str  \n",
    "  ä½¿ç”¨ã™ã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã¾ã™ï¼ˆä¾‹: **llama-3.1-sonar-small-128k-online**ï¼‰ã€‚ã“ã‚Œã«ã‚ˆã‚Šå¿œç­”ã®æ€§èƒ½ã¨èƒ½åŠ›ãŒæ±ºã¾ã‚Šã¾ã™ã€‚\n",
    "\n",
    "- ```temperature``` : float = 0.7  \n",
    "  å¿œç­”ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚0 ã¯æ±ºå®šçš„ã€1 ã¯æœ€ã‚‚ãƒ©ãƒ³ãƒ€ãƒ ãªå‡ºåŠ›ã‚’è¨±å®¹ã—ã¾ã™ã€‚\n",
    "\n",
    "- ```max_tokens``` : int | None = None    \n",
    "  ãƒãƒ£ãƒƒãƒˆè£œå®Œã§ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚\n",
    "\n",
    "è©³ç´°ã¯ Perplexity ã® API ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"PPLX_API_KEY\"):\n",
    "  os.environ[\"PPLX_API_KEY\"] = getpass.getpass(\"Enter API key for Perplexity: \")\n",
    "\n",
    "from langchain_community.chat_models import ChatPerplexity\n",
    "\n",
    "# Create a ChatPerplexity object\n",
    "model = ChatPerplexity(\n",
    "    model=\"llama-3.1-sonar-large-128k-online\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ç’°å¢ƒå¤‰æ•°ã« ```PPLX_API_KEY``` ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚æ‰‹å‹•ã§ API ã‚­ãƒ¼ã‚’æŒ‡å®šã—ãŸã‚Šåˆ¥ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ChatPerplexity(temperature=0, pplx_api_key=\"YOUR_API_KEY_HERE\", model=\"llama-3.1-sonar-large-128k-online\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2024 Nobel Prize in Literature was awarded to South Korean author Han Kang. She was recognized \"for her intense poetic prose that confronts historical traumas and exposes the fragility of [...]\n",
      "\n",
      "[1] https://now.uiowa.edu/news/2024/10/international-writing-program-participant-wins-2024-nobel-prize-literature\n",
      "[2] https://www.nobelprize.org/prizes/literature/2024/prize-announcement/\n",
      "[3] https://www.weforum.org/stories/2024/10/nobel-prize-winners-2024/\n",
      "[4] https://www.nobelprize.org/prizes/literature/2024/han/facts/\n",
      "[5] https://en.wikipedia.org/wiki/2024_Nobel_Prize_in_Literature\n"
     ]
    }
   ],
   "source": [
    "# print out response\n",
    "response = model.invoke(\"Who won the 2024 Nobel Prize in Literature?\")\n",
    "\n",
    "print(response.content)\n",
    "print()\n",
    "# ChatPerplexity stores the sources of knowledge and information in the additional_kwargs[\"citations\"] field.\n",
    "for i, citation in enumerate(response.additional_kwargs[\"citations\"]):\n",
    "    print(f\"[{i+1}] {citation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Together AI\n",
    "\n",
    "Together AI ã¯ã€ç”Ÿæˆ AI ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ãƒ‡ãƒ—ãƒ­ã‚¤ã®ãŸã‚ã®åˆ†æ•£å‹ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã™ã‚‹ä¼æ¥­ã§ã™ã€‚é«˜é€Ÿãªæ¨è«–ã‚¹ã‚¿ãƒƒã‚¯ã‚„ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´æ©Ÿèƒ½ã‚’å‚™ãˆã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "### Together ã®æ¨è«–ã«ã¤ã„ã¦  \n",
    "- æ¥­ç•Œã§æœ€é€Ÿã®æ¨è«–ã‚¹ã‚¿ãƒƒã‚¯ã‚’æä¾›ã—ã€vLLM ã‚ˆã‚Šæœ€å¤§ 4 å€é«˜é€Ÿã¨ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "- Llama-3 70B ã‚’ä½¿ç”¨ã—ãŸå ´åˆã€GPT-4 ã¨æ¯”è¼ƒã—ã¦æœ€å¤§ 11 å€ä½ã‚³ã‚¹ãƒˆã§å‹•ä½œã™ã‚‹ã¨ã—ã¦ã„ã¾ã™ã€‚\n",
    "- API ãƒªã‚¯ã‚¨ã‚¹ãƒˆé‡ã«å¿œã˜ã¦è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "### Together ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«  \n",
    "- ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã•ã‚ŒãŸ AI ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚\n",
    "- FlashAttention-3 ã®ã‚ˆã†ãªæœ€é©åŒ–æŠ€è¡“ã‚’å–ã‚Šå…¥ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "- è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å®Œå…¨ãªæ‰€æœ‰æ¨©ã‚’æä¾›ã—ã¾ã™ã€‚\n",
    "\n",
    "### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–  \n",
    "- FlashAttention-3 ã‚«ãƒ¼ãƒãƒ«ã‚„ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ«ã‚’çµ±åˆã—ãŸæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "- Medusa ã‚„ SpecExec ã®ã‚ˆã†ãªæ¨æ¸¬ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚\n",
    "- æœ€å¤§ã®ç²¾åº¦ã¨æ€§èƒ½ã‚’å¾—ã‚‹ãŸã‚ã®ç‹¬è‡ªã®é‡å­åŒ–æŠ€è¡“ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "\n",
    "### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼\n",
    "- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã¯æ˜ç¤ºçš„ãªåŒæ„ãªã—ã«æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚\n",
    "- ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã«é–¢ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå®Œå…¨ã«åˆ¶å¾¡ã§ãã¾ã™ã€‚\n",
    "\n",
    "### ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«  \n",
    "- Google Gemmaã€Meta ã® **Llama 3.3**ã€**Qwen2.5**ã€Mistral ã® **Mistral/Mixtral** ãªã© 200 ä»¥ä¸Šã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚\n",
    "- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã«ã‚‚å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚\n",
    "- è©³ç´°ã¯ä»¥ä¸‹ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚  \n",
    "  [Together AI Models](https://api.together.xyz/models)\n",
    "\n",
    "### åŸºæœ¬çš„ãªãƒ¢ãƒ‡ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³\n",
    "\n",
    "- ```model_name``` : ```str```  \n",
    "  ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¾ã™ï¼ˆ```model``` ã¨ã‚¨ã‚¤ãƒªã‚¢ã‚¹å¯ï¼‰ã€‚\n",
    "\n",
    "- ```temperature``` : ```float``` = 0.7    \n",
    "  å¿œç­”ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’è¨­å®šã—ã¾ã™ã€‚\n",
    "\n",
    "- ```max_tokens``` : ```int``` | ```None``` = ```None```    \n",
    "  ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚\n",
    "\n",
    "è©³ç´°ã¯ä»¥ä¸‹ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n",
    "https://python.langchain.com/api_reference/together/chat_models/langchain_together.chat_models.ChatTogether.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"TOGETHER_API_KEY\"):\n",
    "  os.environ[\"TOGETHER_API_KEY\"] = getpass.getpass(\"Enter API key for Together AI: \")\n",
    "\n",
    "from langchain_together.chat_models import ChatTogether\n",
    "\n",
    "# Create a ChatPerplexity object\n",
    "model = ChatTogether(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ç’°å¢ƒå¤‰æ•°ã« ```TOGETHER_API_KEY``` ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚æ‰‹å‹•ã§ API ã‚­ãƒ¼ã‚’æŒ‡å®šã—ãŸã‚Šåˆ¥ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ChatTogether(temperature=0, api_key=\"YOUR_API_KEY_HERE\", model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest animal on the planet is the **blue whale** (Balaenoptera musculus). On average, an adult blue whale can grow up to:\n",
      "\n",
      "* 82 feet (25 meters) in length\n",
      "* Weigh around 150-170 tons (136,000-152,000 kilograms)\n",
      "\n",
      "To put that into perspective, that's equivalent to the weight of about 50 elephants or a large building. Blue whales are not only the largest animals on Earth, but they are also the largest kno[...]\n",
      "\n",
      "These massive creatures can be found in all of the world's oceans, from the Arctic to the Antarctic, and are known for their distinctive blue-gray color and massive size. Despite their enormous[...]\n",
      "\n",
      "It's worth noting that while blue whales are the largest animals, there are other large animals on the planet, such as fin whales, humpback whales, and African elephants, which are also impress[...]\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the biggest Animal on The planet?\"\n",
    "\n",
    "# Stream the response instead of invoking it directly\n",
    "response = model.stream(query)\n",
    "\n",
    "# Print the streamed response token by token\n",
    "for token in response:\n",
    "    print(token.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohere\n",
    "\n",
    "Cohere ã¯ä¼æ¥­å‘ã‘ã® AI ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å°‚é–€ã¨ã™ã‚‹ä¸»è¦ä¼æ¥­ã§ã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã‹ã¤åŠ¹ç‡çš„ãªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’æä¾›ã—ã€è‡ªç„¶è¨€èªå‡¦ç†ã‚¿ã‚¹ã‚¯ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "- **å‰µæ¥­:** 2020\n",
    "- **ä¸»è¦æŠ•è³‡å®¶:** Inovia Capitalã€NVIDIAã€Oracleã€Salesforce Ventures\n",
    "- **ã‚·ãƒªãƒ¼ã‚ºCè³‡é‡‘èª¿é”:** 2.7 å„„ãƒ‰ãƒ«ã‚’èª¿é”\n",
    "- **ãƒŸãƒƒã‚·ãƒ§ãƒ³:** ä¼æ¥­ãƒ‹ãƒ¼ã‚ºã«ç‰¹åŒ–ã—ãŸ AI ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚’æä¾›ã™ã‚‹ã“ã¨\n",
    "\n",
    "### ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ« \n",
    "| ãƒ¢ãƒ‡ãƒ«               | èª¬æ˜                                                                                           | ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·     | æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•° |\n",
    "|---------------------|-------------------------------------------------------------------------------------------------------|------------------|-------------------|\n",
    "| command-r-7b        | Command R+ ãƒ¢ãƒ‡ãƒ«ã®å°å‹é«˜é€Ÿç‰ˆã§ã€RAGã€ãƒ„ãƒ¼ãƒ«åˆ©ç”¨ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€å¤šæ®µæ¨è«–ã«å„ªã‚Œã¾ã™ã€‚                       | 128,000 ãƒˆãƒ¼ã‚¯ãƒ³  | 4,000 ãƒˆãƒ¼ã‚¯ãƒ³     |\n",
    "| command-r-plus      | æŒ‡ç¤ºå¾“é †å‹ã®ä¼šè©±ãƒ¢ãƒ‡ãƒ«ã§ã€RAGã€ãƒ„ãƒ¼ãƒ«åˆ©ç”¨ã€è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—æ¨è«–ã«å„ªã‚Œã¾ã™ã€‚                                      | 128,000 ãƒˆãƒ¼ã‚¯ãƒ³  | 4,000 ãƒˆãƒ¼ã‚¯ãƒ³     |\n",
    "| command-r           | é«˜å“è³ªã®è¨€èªã‚¿ã‚¹ã‚¯ã‚„è¤‡é›‘ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆRAGã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã©ï¼‰å‘ã‘ã®ä¼šè©±ãƒ¢ãƒ‡ãƒ«ã€‚                             | 128,000 ãƒˆãƒ¼ã‚¯ãƒ³  | 4,000 ãƒˆãƒ¼ã‚¯ãƒ³     |\n",
    "| command             | æŒ‡ç¤ºå¾“é †å‹ã®ä¼šè©±ãƒ¢ãƒ‡ãƒ«ã§ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šä¿¡é ¼æ€§ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ãŒå‘ä¸Šã—ã¦ã„ã¾ã™ã€‚                              | 4,000 ãƒˆãƒ¼ã‚¯ãƒ³    | 4,000 ãƒˆãƒ¼ã‚¯ãƒ³     |\n",
    "| command-nightly     | å®Ÿé¨“çš„ã«æ¯æ™©æ›´æ–°ã•ã‚Œã‚‹ã‚³ãƒãƒ³ãƒ‰ãƒ¢ãƒ‡ãƒ«ã®ãƒŠã‚¤ãƒˆãƒªãƒ¼ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€‚ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç”¨é€”ã«ã¯æ¨å¥¨ã•ã‚Œã¾ã›ã‚“ã€‚                 | 128,000 ãƒˆãƒ¼ã‚¯ãƒ³  | 4,000 ãƒˆãƒ¼ã‚¯ãƒ³     |\n",
    "| command-light       | ã‚ˆã‚Šå°å‹ã§é«˜é€Ÿãª command ãƒ¢ãƒ‡ãƒ«ã€‚ã»ã¼åŒç­‰ã®æ€§èƒ½ã‚’ä¿ã¡ãªãŒã‚‰é«˜é€ŸåŒ–ã€‚                                           | 4,000 ãƒˆãƒ¼ã‚¯ãƒ³    | 4,000 ãƒˆãƒ¼ã‚¯ãƒ³     |\n",
    "| command-light-nightly | command-light ã®ãƒŠã‚¤ãƒˆãƒªãƒ¼ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€‚å®Ÿé¨“çš„ã‹ã¤å®šæœŸæ›´æ–°ã€‚ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç”¨é€”ã«ã¯æ¨å¥¨ã•ã‚Œã¾ã›ã‚“ã€‚            | 4,000 ãƒˆãƒ¼ã‚¯ãƒ³    | 4,000 ãƒˆãƒ¼ã‚¯ãƒ³     |\n",
    "| c4ai-aya-expanse-8b | å„ªã‚ŒãŸ 8B ãƒãƒ«ãƒãƒªãƒ³ã‚¬ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ 23 è¨€èªã‚’ã‚µãƒãƒ¼ãƒˆã€‚                                                          | 8,000 ãƒˆãƒ¼ã‚¯ãƒ³    | 4,000 ãƒˆãƒ¼ã‚¯ãƒ³     |\n",
    "| c4ai-aya-expanse-32b| é«˜æ€§èƒ½ãª 32B ãƒãƒ«ãƒãƒªãƒ³ã‚¬ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ 23 è¨€èªã‚’ã‚µãƒãƒ¼ãƒˆã€‚                                                      | 128,000 ãƒˆãƒ¼ã‚¯ãƒ³  | 4,000 ãƒˆãƒ¼ã‚¯ãƒ³     |\n",
    "\n",
    "- Cohere ã®ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã¯ä»¥ä¸‹ã‚’å‚ç…§ã—ã¦ãã ã•ã„:  \n",
    "  [Cohere Models](https://docs.cohere.com/docs/models)\n",
    "\n",
    "### åŸºæœ¬çš„ãªãƒ¢ãƒ‡ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³\n",
    "\n",
    "åŸºæœ¬çš„ãª API ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯æ¬¡ã®é€šã‚Šã§ã™:\n",
    "\n",
    "- ```model_name``` : ```str```  \n",
    "  ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¾ã™ï¼ˆ```model``` ã¨ã‚¨ã‚¤ãƒªã‚¢ã‚¹å¯ï¼‰ã€‚\n",
    "\n",
    "- ```temperature``` : ```float``` = 0.7    \n",
    "  å¿œç­”ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’è¨­å®šã—ã¾ã™ã€‚\n",
    "\n",
    "- ```max_tokens``` : ```int``` | ```None``` = ```None```    \n",
    "  ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚\n",
    "\n",
    "è©³ç´°ã¯ä»¥ä¸‹ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n",
    "https://python.langchain.com/api_reference/cohere/chat_models/langchain_cohere.chat_models.ChatCohere.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"COHERE_API_KEY\"):\n",
    "  os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\n",
    "\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "# Create a ChatCohere object\n",
    "model = ChatCohere(\n",
    "    model_name=\"command-r7b-12-2024\", # can be alisaed as 'model'\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ç’°å¢ƒå¤‰æ•°ã« ```COHERE_API_KEY``` ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚æ‰‹å‹•ã§ API ã‚­ãƒ¼ã‚’æŒ‡å®šã—ãŸã‚Šåˆ¥ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ChatCohere(temperature=0, cohere_api_key=\"YOUR_API_KEY_HERE\", model=\"command-r7b-12-2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earth, Wind & Fire, the iconic American band, has had numerous hits throughout their illustrious career, spanning multiple genres such as R&B, soul, funk, jazz, disco, pop, and Afro pop. Here a[...]\n",
      "\n",
      "- \"September\": Released in 1978, \"September\" is arguably the band's most recognizable and popular song. It topped the US Billboard Hot R&B Songs chart and peaked at number eight on the Bill[...]\n",
      "- \"Shining Star\": This was Earth, Wind & Fire's first single to reach number one on the Billboard Hot 100 in 1975. The song won a Grammy Award for Best R&B Performance by a Duo or Group with [...]\n",
      "- \"Boogie Wonderland\": A collaboration with The Emotions, this disco-funk track became a massive hit in 1979. It reached number six on the Billboard Hot 100 and is remembered for its infectio[...]\n",
      "\n",
      "These songs are just a selection of Earth, Wind & Fire's extensive catalog of hits, showcasing their incredible musical range and enduring popularity."
     ]
    }
   ],
   "source": [
    "query = \"What are the Biggest hit songs of Earth, Wind & Fire?\"\n",
    "\n",
    "# Stream the response instead of invoking it directly\n",
    "response = model.stream(query)\n",
    "\n",
    "# Print the streamed response token by token\n",
    "for token in response:\n",
    "    print(token.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upstage\n",
    "\n",
    "Upstage ã¯éŸ“å›½ã®ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã§ã€ç‰¹ã«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ AI ã‚’å°‚é–€ã¨ã—ã¦ã„ã¾ã™ã€‚ã‚³ã‚¹ãƒˆåŠ¹ç‡ã¨é«˜æ€§èƒ½ã‚’ä¸¡ç«‹ã—ãŸ AI ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä¼æ¥­å‘ã‘ã«æä¾›ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "- **ä¸»åŠ›è£½å“: Solar LLM**  \n",
    "  Upstage ã®ä¸»è¦ãªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã€é€Ÿåº¦ã€åŠ¹ç‡æ€§ã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã«å„ªã‚Œã¦ã„ã¾ã™ã€‚Depth-Up Scalingï¼ˆDUSï¼‰æŠ€è¡“ã‚’ç”¨ã„ã¦æ€§èƒ½ã‚’æœ€å¤§åŒ–ã—ã¾ã™ã€‚\n",
    "\n",
    "- **Document AI ãƒ‘ãƒƒã‚¯**  \n",
    "  é«˜åº¦ãª OCR æŠ€è¡“ã‚’å‚™ãˆãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã§ã€è¤‡é›‘ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æ­£ç¢ºã«æƒ…å ±ã‚’æŠ½å‡ºãƒ»ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã—ã¾ã™ã€‚\n",
    "\n",
    "- **AskUp Seargest**  \n",
    "  AskUp ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ç‰ˆã§ã€å€‹åˆ¥åŒ–ã•ã‚ŒãŸæ¤œç´¢ãƒ»æ¨è–¦ã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚\n",
    "\n",
    "Upstage ã¯ä¼æ¥­ã®è‡ªå‹•åŒ–ã¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®åŠ¹ç‡åŒ–ã€AI ã«ã‚ˆã‚‹ã‚¤ãƒ³ã‚µã‚¤ãƒˆæä¾›ã‚’æ”¯æ´ã—ã¾ã™ã€‚\n",
    "\n",
    "### ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ« \n",
    "\n",
    "| ãƒ¢ãƒ‡ãƒ«        | èª¬æ˜                                                                                           | ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·     | å­¦ç¿’ãƒ‡ãƒ¼ã‚¿           |\n",
    "|--------------|------------------------------------------------------------------------------------------------|------------------|----------------------|\n",
    "| solar-pro    | ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºå‘ã‘ã®é«˜æ€§èƒ½ LLMã€‚æŒ‡ç¤ºå¾“é †æ€§ã¨ HTML ã‚„ Markdown ã®ã‚ˆã†ãªæ§‹é€ åŒ–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡¦ç†ã«å„ªã‚Œã€å¤šè¨€èªæ€§èƒ½ã‚‚é«˜ã„ã€‚                 | -                | -                    |\n",
    "| solar-mini   | 10.7B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å°å‹ãƒ¢ãƒ‡ãƒ«ã§ã€è‹±èªãƒ»éŸ“å›½èªãƒ»æ—¥æœ¬èªã«å¯¾å¿œã€‚ãƒ“ã‚¸ãƒã‚¹ç”¨é€”ã«é©ã—ãŸæŒ‡ç¤ºå¾“é †å‹ã®ä¼šè©±ãƒ¢ãƒ‡ãƒ«ã€‚                                | -                | -                    |\n",
    "| solar-mini-ja| æ—¥æœ¬èªå‡¦ç†ã«å¼·åŒ–ã•ã‚ŒãŸ Solar Miniã€‚æ—¥æœ¬èªã€è‹±èªã€éŸ“å›½èªã‚’ã‚µãƒãƒ¼ãƒˆã€‚                                                       | -                | -                    |\n",
    "\n",
    "- è©³ç´°ã¯ä»¥ä¸‹ã‚’å‚ç…§ã—ã¦ãã ã•ã„:  \n",
    "  [Upstage Models](https://console.upstage.ai/docs/capabilities/chat)\n",
    "\n",
    "### åŸºæœ¬çš„ãªãƒ¢ãƒ‡ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³\n",
    "\n",
    "- ```model_name``` : ```str```  \n",
    "  ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¾ã™ï¼ˆ```model``` ã¨ã‚¨ã‚¤ãƒªã‚¢ã‚¹å¯ï¼‰ã€‚\n",
    "\n",
    "- ```temperature``` : ```float``` = 0.7    \n",
    "  å¿œç­”ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’è¨­å®šã—ã¾ã™ã€‚\n",
    "\n",
    "- ```max_tokens``` : ```int``` | ```None``` = ```None```    \n",
    "  ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚\n",
    "\n",
    "è©³ç´°ã¯ä»¥ä¸‹ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n",
    "https://python.langchain.com/api_reference/upstage/chat_models/langchain_upstage.chat_models.ChatUpstage.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"UPSTAGE_API_KEY\"):\n",
    "  os.environ[\"UPSTAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Upstage: \")\n",
    "\n",
    "from langchain_upstage import ChatUpstage \n",
    "\n",
    "# Create a ChatUpstage object\n",
    "model = ChatUpstage(\n",
    "    model_name=\"solar-mini\", # can be alisaed as 'model'\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ç’°å¢ƒå¤‰æ•°ã« ```UPSTAGE_API_KEY``` ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚æ‰‹å‹•ã§ API ã‚­ãƒ¼ã‚’æŒ‡å®šã—ãŸã‚Šåˆ¥ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ChatUpstage(temperature=0, upstage_api_key=\"YOUR_API_KEY_HERE\", model=\"solar-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the computer scientist go broke?\n",
      "\n",
      "He invested in a company that was developing a new programming language, but it turned out to be a \"dead end\" language."
     ]
    }
   ],
   "source": [
    "query = \"Tell me one joke about Computer Science\"\n",
    "\n",
    "# Stream the response instead of invoking it directly\n",
    "response = model.stream(query)\n",
    "\n",
    "# Print the streamed response token by token\n",
    "for token in response:\n",
    "    print(token.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open LLM Leaderboard\n",
    "\n",
    "Open LLM Leaderboard ã¯ Hugging Face ãŒãƒ›ã‚¹ãƒˆã™ã‚‹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ä¸»å°ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã¨ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®è©•ä¾¡ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ã‚¹ã‚³ã‚¢ã€ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã‚’æä¾›ã—ã€ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒã«åˆ©ç”¨ã§ãã¾ã™ã€‚\n",
    "\n",
    "Open LLM Leaderboard ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€Hugging Face ã‚„ Ollama ã¨ã„ã£ãŸãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã«çµ±åˆã—ã‚„ã™ã„é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
    "\n",
    "è©³ç´°ã¯ä»¥ä¸‹ã‚’å‚ç…§ã—ã¦ãã ã•ã„:  \n",
    "[Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vellum LLM Leaderboard\n",
    "\n",
    "Vellum LLM Leaderboard ã¯å•†ç”¨ãƒ»ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ä¸»è¦ãªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’æ€§èƒ½ã€ä¾¡æ ¼ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºãªã©ã®è¦³ç‚¹ã‹ã‚‰æ¯”è¼ƒã™ã‚‹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«é¸å®šã«å½¹ç«‹ã¤ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’æä¾›ã—ã¾ã™ã€‚\n",
    "\n",
    "è©³ç´°ã¯ä»¥ä¸‹ã‚’å‚ç…§ã—ã¦ãã ã•ã„:  \n",
    "[Vellum LLM Leaderboard](https://www.vellum.ai/llm-leaderboard)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
