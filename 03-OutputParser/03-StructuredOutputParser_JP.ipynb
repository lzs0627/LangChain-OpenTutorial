{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Output Parser\n",
    "\n",
    "- 著者: [Yoolim Han](https://github.com/hohosznta)\n",
    "- ピアレビュー: [ranian963](https://github.com/ranian963), [asummerz](https://github.com/asummerz)\n",
    "- 校正: [BokyungisaGod](https://github.com/BokyungisaGod)\n",
    "- これは [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial) の一部です\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/03-OutputParser/03-StructuredOutputParser.ipynb)\n",
    "[![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/03-OutputParser/03-StructuredOutputParser.ipynb)\n",
    "\n",
    "## 概要\n",
    "\n",
    "```StructuredOutputParser```は、大規模言語モデル(LLM)の応答を辞書構造にフォーマットし、複数のフィールドをキー/値のペアとして返すことができる貴重なツールです。\n",
    "PydanticおよびJSONパーサーは堅牢な機能を提供しますが、```StructuredOutputParser```は、パラメータ数が少ないローカルモデルなど、性能が低いモデルに特に効果的です。GPTやClaudeのような高度なモデルと比較して知能が低いモデルには特に有益です。\n",
    "```StructuredOutputParser```を活用することで、開発者はパラメータ数が少ないモデルを使用している場合でも、さまざまなLLMアプリケーション全体でデータの整合性と一貫性を維持できます。\n",
    "\n",
    "### 目次\n",
    "\n",
    "- [概要](#概要)\n",
    "- [環境設定](#環境設定)\n",
    "- [StructuredOutputParserの実装](#StructuredOutputParserの実装)\n",
    "\n",
    "### 参考文献\n",
    "\n",
    "- [LangChain ChatOpenAI APIリファレンス](https://python.langchain.com/docs/integrations/chat/openai/)\n",
    "- [LangChain Structured output parser](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.structured.StructuredOutputParser.html#langchain.output_parsers.structured.StructuredOutputParser)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境設定\n",
    "\n",
    "環境をセットアップします。詳細については[環境設定](https://wikidocs.net/257836)を参照してください。\n",
    "\n",
    "**[注意]**\n",
    "- ```langchain-opentutorial```は、チュートリアル用の使いやすい環境設定と、便利な関数やユーティリティを提供するパッケージです。\n",
    "- 詳細については[```langchain-opentutorial```](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi)をご確認ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なパッケージをインストール\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_openai\",\n",
    "        \"langchain_community\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "環境変数が正常に設定されました。\n"
     ]
    }
   ],
   "source": [
    "# 環境変数を設定\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"03-StructuredOutputParser\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "あるいは、```.env```ファイルに```OPENAI_API_KEY```を設定して読み込むこともできます。\n",
    "\n",
    "[注意] 前の手順で```OPENAI_API_KEY```を既に設定している場合、これは不要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ```StructuredOutputParser```の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jJN9mDnR2Mp"
   },
   "source": [
    "### ```ResponseSchema```と```StructuredOutputParser```の使用\n",
    "*   ```ResponseSchema```クラスを使用して応答スキーマを定義し、ユーザーの質問への回答と、使用されたソース(ウェブサイト)の```description```を含めます。\n",
    "\n",
    "*   ```response_schemas```を使用して```StructuredOutputParser```を初期化し、定義された応答スキーマに従って出力を構造化します。\n",
    "\n",
    "**[注意]**\n",
    "ローカルモデルを使用する場合、Pydanticパーサーが正常に動作しないことが頻繁にあります。そのような場合、```StructuredOutputParser```を使用することが良い代替解決策となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "udCLc1sDR01b"
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "# ユーザーの質問への応答\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"answer\", description=\"ユーザーの質問への回答\"),\n",
    "    ResponseSchema(\n",
    "        name=\"source\",\n",
    "        description=\"ユーザーの質問に回答するために使用された`source`で、`ウェブサイトURL`である必要があります。\",\n",
    "    ),\n",
    "]\n",
    "# 応答スキーマに基づいて構造化出力パーサーを初期化\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMgOch7KTcNh"
   },
   "source": [
    "### プロンプトへの応答スキーマの埋め込み\n",
    "\n",
    "```PromptTemplate```を作成して、ユーザーの質問をフォーマットし、構造化出力の解析指示を埋め込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GeamtrixZMUJ"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "# フォーマット指示を解析\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    # ユーザーの質問にできるだけ適切に回答するテンプレートを設定\n",
    "    template=\"ユーザーの質問にできるだけ適切に回答してください。\\n{format_instructions}\\n{question}\",\n",
    "    # 'question'を入力変数として使用\n",
    "    input_variables=[\"question\"],\n",
    "    # 'format_instructions'を部分変数として使用\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```ChatOpenAI```との統合と```Chain```の実行\n",
    "\n",
    "```PromptTemplate```、```ChatOpenAI```モデル、```StructuredOutputParser```を```chain```に結合します。最後に、特定の```question```で```chain```を実行して結果を生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cSWnxIdOZRWy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '世界最大の砂漠は南極砂漠です。',\n",
       " 'source': 'https://www.worldatlas.com/articles/what-is-the-largest-desert-in-the-world.html'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0)  # ChatOpenAIモデルを初期化\n",
    "\n",
    "chain = prompt | model | output_parser  # プロンプト、モデル、出力パーサーを接続\n",
    "\n",
    "# 「世界最大の砂漠は何ですか?」という質問をする\n",
    "chain.invoke({\"question\": \"世界最大の砂漠は何ですか?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IODAUdNHZZQ-"
   },
   "source": [
    "### ストリーミング出力の使用\n",
    "\n",
    "```chain.stream```メソッドを使用して、「サッカーチームには何人のプレーヤーがいますか?」という```question```へのストリーミング応答を受け取ります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "gzGhr-8DZaZu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '標準的なサッカーチームは、フィールド上に一度に11人のプレーヤーがいます。', 'source': 'https://www.fifa.com/who-we-are/news/what-are-the-rules-of-football-2040008'}\n"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"question\": \"サッカーチームには何人のプレーヤーがいますか?\"}):\n",
    "    # 出力をストリーミング\n",
    "    print(s)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
